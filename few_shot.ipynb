{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "743ad24e-7540-40b4-bdc9-986e3f4dd723",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2154it [2:38:47,  4.42s/it]\n"
     ]
    }
   ],
   "source": [
    "from utils import llms\n",
    "from utils import prompts\n",
    "import yaml\n",
    "import jsonlines\n",
    "import json\n",
    "import toml\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "from together import Together\n",
    "\n",
    "# # open config\n",
    "# config_path = \"./configs/config_zero-shot.yaml\"\n",
    "# with open(config_path, 'r') as f:\n",
    "#     content = f.read()\n",
    "# config = yaml.safe_load(content)\n",
    "\n",
    "input_file = \"/Users/jjr/PycharmProjects/RARR-rep/input/processed/fava_input_processed_2.jsonl\"\n",
    "output_file = \"./output/fava_zero_shot_output_dsr1_2.jsonl\"\n",
    "# model=\"flywheel25/meta-llama/Llama-3.2-1B-Instruct-7695e079\"\n",
    "# model=\"flywheel25/meta-llama/Meta-Llama-3.1-8B-Instruct-Reference-38ef9c4e\"\n",
    "# model=\"flywheel25/meta-llama/Meta-Llama-3.1-70B-Instruct-Reference-bbae0ef2\"\n",
    "model=\"deepseek-ai/DeepSeek-R1\"\n",
    "\n",
    "# instantiate llm\n",
    "# llm = llms.instantiate_llm(config)\n",
    "api_key = \"\"\n",
    "client = Together(api_key=api_key)\n",
    "\n",
    "\n",
    "# continue_flag = False  # or read it from some config or CLI argument\n",
    "# mode = \"a\" if continue_flag else \"w\"\n",
    "# continue_idx = 2403\n",
    "mode = 'w'\n",
    "# load input data\n",
    "# iterate over the input sentences\n",
    "with jsonlines.open(input_file) as input_reader, \\\n",
    "        open(output_file, mode, encoding=\"utf-8\") as output_writer:\n",
    "    for i, item in enumerate(tqdm.tqdm(input_reader), start=1):\n",
    " \n",
    "        # if i < continue_idx:\n",
    "        #     continue\n",
    "        \n",
    "        # start timing the iteration\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        output_data = item\n",
    "\n",
    "        sentence = output_data[\"decon_sentence\"]\n",
    "\n",
    "        # Populate the prompt with the required data\n",
    "        prompt_data = {\"sentence\": sentence, \"example_json\": \"{\\\"sentence_score\\\": 1, \\\"query_score\\\": 0}\"}\n",
    "\n",
    "        # load prompt config\n",
    "        path_to_config = f\"./prompts/config_zero-shot-1_Llama.toml\"\n",
    "\n",
    "        try:\n",
    "            with open(path_to_config) as f:\n",
    "                prompt_config = toml.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(\"File not found.\")\n",
    "            exit()\n",
    " \n",
    "\n",
    "        # create llm messages\n",
    "        messages = prompts.create_llm_messages(prompt_config, prompt_data)\n",
    "        \n",
    "        # prompt llm\n",
    "        # response = llm.generate(messages)\n",
    "\n",
    "        response = client.chat.completions.create(model=model, messages=messages)\n",
    " \n",
    "        # parse response\n",
    "        response = response.choices[0].message.content\n",
    "\n",
    "        data = {}\n",
    "        for line in response.splitlines():\n",
    "            if \"=\" in line:\n",
    "                key, value = line.split(\"=\", 1)\n",
    "            elif \":\" in line:\n",
    "                key, value = line.split(\":\", 1)\n",
    "            else:\n",
    "                continue\n",
    "            data[key.strip()] = value.strip()\n",
    "\n",
    "\n",
    "        # Convert numeric values from strings to integers if necessary\n",
    "        try:\n",
    "            has_facts = int(data.get(\"has_facts\", 0))\n",
    "        except ValueError:\n",
    "            print(\"error parsing has_facts\")\n",
    "            has_facts = 0\n",
    "        try:\n",
    "            has_errors = int(data.get(\"has_errors\", 0))\n",
    "        except ValueError:\n",
    "            print(\"error parsing has_errors\")\n",
    "            has_errors = 0\n",
    "        reason = data.get(\"reason\", \"\")\n",
    "        data = {'response': response, 'has_facts': has_facts, 'has_errors': has_errors, 'reason': reason}\n",
    "\n",
    "        iteration_time = time.perf_counter() - start_time\n",
    "        output_data[\"zs_iteration_time\"] = iteration_time\n",
    "\n",
    "        # add queries to output data\n",
    "        output_data[\"zs_response\"] = data\n",
    "\n",
    "        # write 1 line of output\n",
    "        output_writer.write(json.dumps(output_data, ensure_ascii=False) + \"\\n\")\n",
    "        output_writer.flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1aee55-25b6-4329-9cb6-9914d7cbee78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rarr-rep",
   "language": "python",
   "name": "rarr-rep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
