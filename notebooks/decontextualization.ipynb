{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f18a85d1-4c0f-46ee-9b1b-332dc599b2b7",
   "metadata": {},
   "source": [
    "# Dataset prep for Gemini - FAVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8ac76669-e09c-4427-ad2c-5c80d9ac64a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load by sentence\n",
    "sentences_for_decontext = []\n",
    "fava_bs_all = []\n",
    "with open('./data/rarr-input_fava_bs_all.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        fava_bs_all.append(json.loads(line))\n",
    "        \n",
    "# load by passage\n",
    "fava_bp_all = []\n",
    "with open('/Users/jjr/PycharmProjects/RARR/data/rarr-input_fava-gold.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        fava_bp_all.append(json.loads(line))\n",
    "\n",
    "        \n",
    "# iterate over sentences, add passage to sentence dictionary\n",
    "for s in fava_bs_all:\n",
    "    passage_id = s['input_info']['cid']\n",
    "    passage = fava_bp_all[passage_id]['input_info']['claim']\n",
    "    s['input_info']['passage'] = passage\n",
    "    # rename 'claim' to 'passage'\n",
    "    s['input_info']['sentence'] = s['input_info'].pop('claim')\n",
    "    sentences_for_decontext.append(s)\n",
    "\n",
    "# write output\n",
    "# Write to a .jsonl file\n",
    "with open('./data/rarr_sentences_for_decon.jsonl', 'w') as f:\n",
    "    for entry in sentences_for_decontext:\n",
    "        f.write(json.dumps(entry) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8f98ed-6ac2-476f-b10e-f066d3574d7e",
   "metadata": {},
   "source": [
    "# Dataset prep for Gemini - WikiBib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb44a7e7-a79f-4126-950d-eabf07468543",
   "metadata": {},
   "source": [
    "## -> Helper classes / Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0173049d-55f3-460c-ac93-49bc956be57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sympy.physics.units import current\n",
    "from torch.utils.data import Subset\n",
    "# import dataloader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json, jsonlines\n",
    "import tqdm\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import nltk\n",
    "\n",
    "\n",
    "class WikibibDataset(Dataset):\n",
    "    def __init__(self, json_file=\"./data/wikibib_halluc/labeled/ChatGPT.jsonl\",\n",
    "                       artificial_fact_path=None):\n",
    "        if artificial_fact_path is not None:\n",
    "            self.artificial_facts = []\n",
    "            self.artificial_evidence = []\n",
    "            with open(artificial_fact_path) as f:\n",
    "                for line in f:\n",
    "                    item = json.loads(line)\n",
    "                    question_list =  item[\"result\"][\"question_gen\"]['questions']\n",
    "                    self.artificial_facts.append(question_list)\n",
    "                    self.artificial_evidence.append(item[\"result\"][\"search\"]['used_evidence'])\n",
    "        else:\n",
    "            self.artificial_facts = None\n",
    "            self.artificial_evidence = None\n",
    "\n",
    "        self.data = []\n",
    "        with open(json_file) as f:\n",
    "            for line in f:\n",
    "                self.data.append(json.loads(line))\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        current_sample = self.data[idx]\n",
    "        question  = current_sample[\"input\"]\n",
    "        llm_reply = current_sample[\"output\"]\n",
    "        topic = current_sample[\"topic\"]\n",
    "        # annotations = current_sample[\"annotations\"]# [\"human-atomic-facts\"]\n",
    "        # annotations represent the list of sentences, where each sentence\n",
    "        #   is a dictionary with the \"text\" and \"label\" keys, Labels being \"S\", \"NS\" or \"IR\"\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"llm_reply\": llm_reply,\n",
    "            \"topic\": topic,\n",
    "            # \"annotations\": annotations,\n",
    "        }\n",
    "\n",
    "    def load_rarr_results(self, output_file):\n",
    "        finished_results = {}\n",
    "        for l in jsonlines.open(output_file):\n",
    "            finished_results[l[\"llm_reply\"]] = l[\"result\"]\n",
    "        return finished_results\n",
    "\n",
    "    def get_claim(self, idx):\n",
    "        current_sample = self.data[idx]\n",
    "        return current_sample[\"output\"]\n",
    "\n",
    "    def get_gt_facts(self, idx):\n",
    "        \"\"\"\n",
    "        Get the facts for a given index\n",
    "        Args:\n",
    "            idx:\n",
    "\n",
    "        Returns:\n",
    "            evidence_list: List of evidence\n",
    "            label_list: List of labels\n",
    "            topic: The name of the Bibliography topic\n",
    "        \"\"\"\n",
    "        item = self.data[idx]\n",
    "        # print(item.keys(), item['topic'])\n",
    "\n",
    "        label_mapping = {\"S\": 1, \"NS\": 0, \"IR\": -1}\n",
    "        evidence_list, label_list = [], []\n",
    "\n",
    "        # if annotations are null, return empty lists\n",
    "        if item[\"annotations\"] is not None:\n",
    "            # print(item[\"annotations\"])\n",
    "            for sentence in item[\"annotations\"]:\n",
    "                atomic_facts = sentence[\"human-atomic-facts\"]\n",
    "                #print(\"Atomic Facts: \", atomic_facts)\n",
    "                if atomic_facts is not None:\n",
    "                    for fact in atomic_facts:\n",
    "                        evidence_list.append(fact[\"text\"])\n",
    "                        label_list.append(label_mapping[fact[\"label\"]])\n",
    "        return evidence_list, label_list, item[\"topic\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1b5d4e-4572-4e6c-b0ab-748110db7a96",
   "metadata": {},
   "source": [
    "## -> Prepping for decontextualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0ce87228-8c00-45fb-abb6-2a01f16cb6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 183/183 [00:00<00:00, 5988.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# load wikibib dataset\n",
    "dset = WikibibDataset(json_file='/Users/jjr/Documents/data/rarr-rep/wikiBib/ChatGPT.jsonl', artificial_fact_path=None)\n",
    "\n",
    "dset_with_sentences = []\n",
    "\n",
    "# iterate over inputs\n",
    "for i in tqdm.tqdm(range(len(dset))):\n",
    "    # retrieve the passage\n",
    "    llm_passage = dset.get_claim(i)\n",
    "    # split passage into sentences\n",
    "    claim_list = nltk.sent_tokenize(llm_passage)\n",
    "    # retrieve the entire sample\n",
    "    wiki_whole_sample = dset[i]\n",
    "    # add sentences to the sample\n",
    "    wiki_whole_sample['sentences'] = claim_list\n",
    "    # add to list\n",
    "    dset_with_sentences.append(wiki_whole_sample)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540750a3-16bd-4a55-b251-4f62815c88de",
   "metadata": {},
   "source": [
    "## -> Save as jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "01506364-0d32-44e5-a0e5-edd7580f61c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to a .jsonl file\n",
    "with open('./data/wikibib_with_sentences.jsonl', 'w') as f:\n",
    "    for entry in dset_with_sentences:\n",
    "        f.write(json.dumps(entry) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rarr-rep",
   "language": "python",
   "name": "rarr-rep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
