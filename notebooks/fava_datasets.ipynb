{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed38cbc-e559-40c6-b1cc-5f0165bc3603",
   "metadata": {},
   "source": [
    "# Import FAVA annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62caba24-3619-4fc1-9180-ce4b0dfce84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/Users/jjr/Documents/data/rarr-rep/input/fava/annotations.json\", 'r') as file:\n",
    "    annotations = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d85154-c765-48b3-aa25-59b9f239adbb",
   "metadata": {},
   "source": [
    "# Create by passage Dataset from annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31fbafcb-4486-45c8-82b2-8febed4a83e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from utilities import create_error_count_dict, extract_gold_answer\n",
    "\n",
    "overwrite_existing = False\n",
    "\n",
    "# raar input file will be a list of dictionaries\n",
    "dataset_bp = [{\"input_info\":{}} for row in annotations]\n",
    "\n",
    "# iterate over each row in annotated\n",
    "for idx, row in enumerate(annotations):\n",
    "    # -> add id\n",
    "    dataset_bp[idx][\"input_info\"][\"id\"] = idx\n",
    "    \n",
    "    # -> add claim\n",
    "    dataset_bp[idx][\"input_info\"][\"claim\"] = annotations[idx][\"output\"]\n",
    "    \n",
    "    # -> add gold\n",
    "    dataset_bp[idx][\"input_info\"][\"gold\"] = extract_gold_answer(annotations[idx][\"output\"])\n",
    "    \n",
    "    # -> add error count dictionary for this passage\n",
    "    annotated_text = annotations[idx][\"annotated\"]\n",
    "    dataset_bp[idx][\"input_info\"][\"error_count_dict\"] = create_error_count_dict(annotated_text)\n",
    "    \n",
    "    # -> add flag to indicate if sample has an error\n",
    "    error_counts_dict = dataset_bp[idx][\"input_info\"][\"error_count_dict\"]\n",
    "    # check if any errors for this sample\n",
    "    has_error = any(value > 0 for value in error_counts_dict.values())\n",
    "    dataset_bp[idx]['input_info']['has_error'] = has_error\n",
    "\n",
    "if overwrite_existing:\n",
    "    # path to the output file\n",
    "    filename = '/Users/jjr/Documents/data/rarr-rep/input/fava/rarr-input_fava_bp.jsonl'\n",
    "    # write jsonlines file\n",
    "    with open(filename, 'w') as f:\n",
    "        for obj in dataset_bp:\n",
    "            json_line = json.dumps(obj)  \n",
    "            f.write(json_line + '\\n')     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f479a5b1-3c50-4a48-b914-71d73fffb6ce",
   "metadata": {},
   "source": [
    "# Create by Sentence Dataset from passages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb15a5c-c45e-4fdb-a935-36dcb2893918",
   "metadata": {},
   "source": [
    "## -> helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8c92462-7926-4779-9656-1cbf126ce5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def cleanup_lone_annot(sentences):\n",
    "    cleaned_sentences = []\n",
    "    buffer = \"\"\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        # Match any tag within <> up to 20 characters, alone on a line\n",
    "        if re.fullmatch(r\"<[^>]{1,20}>\", sentence.strip()):\n",
    "            tag = sentence.strip()\n",
    "            \n",
    "            if tag.startswith(\"</\"):  # Move to the end of the previous sentence\n",
    "                if cleaned_sentences:\n",
    "                    cleaned_sentences[-1] += f\" {tag}\"\n",
    "                else:\n",
    "                    buffer += tag  # If there's no previous sentence, buffer it\n",
    "            else:  # Move to the beginning of the next sentence\n",
    "                buffer += tag + \" \"\n",
    "                \n",
    "        else:\n",
    "            # Append buffer if it exists to the beginning of the current sentence\n",
    "            sentence = buffer + sentence\n",
    "            buffer = \"\"  # Clear buffer after use\n",
    "            cleaned_sentences.append(sentence)\n",
    "    \n",
    "    return cleaned_sentences\n",
    "\n",
    "\n",
    "def cleanup_leading_end_annot(sentences):\n",
    "    # Define a regex pattern to match an optional prefix (1-2 chars) and ending tag at the start\n",
    "    pattern = re.compile(r'^[^a-zA-Z0-9]{0,2}</[^>]+>')\n",
    "\n",
    "    cleaned_sentences = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        # Check if the sentence starts with an annotation end or short prefix + annotation end\n",
    "        match = pattern.match(sentence)\n",
    "        if match:\n",
    "            # Extract the annotation and remove it from the current sentence\n",
    "            annotation = match.group(0)\n",
    "            sentence = sentence[len(annotation):].strip()\n",
    "            # Append the annotation to the end of the previous sentence if there is one\n",
    "            if cleaned_sentences:\n",
    "                cleaned_sentences[-1] += annotation\n",
    "        # Add the cleaned or unchanged sentence to the list\n",
    "        cleaned_sentences.append(sentence)\n",
    "\n",
    "    return cleaned_sentences\n",
    "\n",
    "def cleanup_dangling_start_annot(sentences):\n",
    "    cleaned_sentences = []\n",
    "    pending_annotation = \"\"\n",
    "\n",
    "    # Define regex pattern for matching opening <annotation> at the end of a sentence\n",
    "    open_annotation_pattern = re.compile(r'(<[^/][^>]{0,19}>)(\\S{0,2}\\s*\\.?)$')\n",
    "     \n",
    "    for sentence in sentences:\n",
    "        # Prepend the pending annotation if present\n",
    "        if pending_annotation:\n",
    "            sentence = pending_annotation + \" \" + sentence\n",
    "            pending_annotation = \"\"\n",
    "\n",
    "        # Check for an opening annotation at the end of the sentence with optional trailing characters\n",
    "        match = open_annotation_pattern.search(sentence)\n",
    "        \n",
    "        if match:\n",
    "            annotation = match.group(1)\n",
    "            trailing_chars = match.group(2).strip()\n",
    "            \n",
    "            # Remove the annotation and store it for the next sentence\n",
    "            sentence = sentence[:match.start()] + trailing_chars\n",
    "            pending_annotation = annotation\n",
    "\n",
    "        cleaned_sentences.append(sentence.strip())\n",
    "\n",
    "    \n",
    "\n",
    "    return [sent for sent in cleaned_sentences if len(sent) > 0]\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "def process_annotations(text: str) -> Tuple[str, Dict[str, int]]:\n",
    "    # Regex to find annotations and their types\n",
    "    annotation_pattern = r\"<(\\w+)>(.*?)</\\1>\"\n",
    "\n",
    "    # Dictionary to count annotations\n",
    "    annotation_counts = defaultdict(int)\n",
    "\n",
    "    # Function to remove annotation markers but keep the content\n",
    "    def remove_annotation(match):\n",
    "        annotation_type = match.group(1)\n",
    "        content = match.group(2)\n",
    "        annotation_counts[annotation_type] += 1\n",
    "        return content  # Return the content without the markers\n",
    "\n",
    "    # Clean the text by removing annotations\n",
    "    cleaned_text = re.sub(annotation_pattern, remove_annotation, text)\n",
    "\n",
    "    # Convert defaultdict to dict for return\n",
    "    annotation_counts = dict(annotation_counts)\n",
    "\n",
    "    # # Check if any annotations were found\n",
    "    # if not annotation_counts:\n",
    "    #     return text, annotation_counts  # Return the original text if no annotations were found\n",
    "\n",
    "    return cleaned_text, annotation_counts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def remove_irrelevant_annotations(text: str, keep_errors: List[str])-> str:\n",
    "    # Regex pattern to match any tag independently, capturing the type\n",
    "    tag_pattern = re.compile(r'</?([^<>]+)>')\n",
    "\n",
    "    # Function to replace tags based on the type\n",
    "    def replace_tags(match):\n",
    "        # remove '/'\n",
    "        tag_type = match.group(1).replace('/', '') \n",
    "        # Check if the tag type is in the list of errors to keep\n",
    "        if tag_type in keep_errors:\n",
    "            return match.group(0)  \n",
    "        else:\n",
    "            return '' \n",
    "\n",
    "    # Replace tags in the text using the replace_tags function\n",
    "    return re.sub(tag_pattern, replace_tags, text)\n",
    "\n",
    "\n",
    "\n",
    "# import re\n",
    "# from collections import Counter\n",
    "\n",
    "# def count_error_annotations(text):\n",
    "#     \"\"\"\n",
    "#     Counts the number of error annotations in the given text.\n",
    "\n",
    "#     Args:\n",
    "#         text (str): The text containing error annotations.\n",
    "\n",
    "#     Returns:\n",
    "#         dict: A dictionary with error types as keys and their counts as values.\n",
    "#     \"\"\"\n",
    "#     # Define the error types you're interested in\n",
    "#     error_types = ['contradictory', 'relation', 'entity', 'invented', 'unverifiable', 'subjective']\n",
    "    \n",
    "#     # Initialize a counter for each error type\n",
    "#     error_counts = Counter({error_type: 0 for error_type in error_types})\n",
    "\n",
    "#     # Iterate over each error type and count occurrences\n",
    "#     for error_type in error_types:\n",
    "#         # Regex pattern to find tags, case-insensitive\n",
    "#         pattern = fr'<{error_type}>.*?</{error_type}>'\n",
    "#         matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "#         error_counts[error_type] += len(matches)\n",
    "    \n",
    "#     return dict(error_counts)\n",
    "\n",
    "\n",
    "def remove_mark_tags(text):\n",
    "    # This regex pattern matches '<mark>' followed by any content (non-greedy), up to '</mark>'\n",
    "    pattern = r'<mark>.*?</mark>'\n",
    "    # The re.DOTALL flag allows '.' to match newline characters as well\n",
    "    cleaned_text = re.sub(pattern, '', text, flags=re.DOTALL)\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c509cc-f0bc-427e-b94a-12fa3cd0b7a8",
   "metadata": {},
   "source": [
    "## -> Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "346aa5fb-d777-44e2-93dc-ad843206c4ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Amdavad ni Gufa is an underground art gallery in Ahmedabad, Gujarat, India. It was designed by the renowned architect Balkrishna Doshi and <subjective><delete>is located on the banks of the Sabarmati River.</delete></subjective>The gallery is known for its unique design, which resembles a cave or a cave-like structure. It is made up of twisted, curved, and uneven shapes that create an immersive and complex experience for visitors.Amdavad ni Gufa is home to various exhibitions and installations that showcase the works of local and international artists. Its aim is to provide a platform for contemporary art and encourage creativity and innovation in the field.<unverifiable><delete>The gallery is a popular tourist attraction and has won several architecture and design awards.</delete></unverifiable> <relation><delete>It is open to the public every day except Monday and admission is free.</delete><mark>it is open to the public every day except monday and public holidays and admission is free.</mark></relation>'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations[1]['annotated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1310710-9d2e-4313-aa8a-00ad0f49e5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exceptions:\n",
    "# -> annotation is tokenized as a sentence -> merge with prev or next\n",
    "# -> annotation start is at end of a sentence -> merge with next\n",
    "# -> annotation end is at start of a sentence -> merge with previous\n",
    "# -> annotation spans multiple sentences (do nothing at this point)\n",
    "# -> two sentences tokenized as one sentence\n",
    "# other -> [21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d9d406d-5046-400a-ae05-8a030f3f39c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['prompt', 'output', 'annotated', 'subject', 'dataset', 'model'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "826ac1d8-c542-4a87-8a87-059623478af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "error_types = ['contradictory', 'relation', 'entity', 'invented', 'unverifiable', 'subjective']\n",
    "\n",
    "spacy_nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "\n",
    "processed_sentences = []\n",
    "concat_passages = []\n",
    "id = 0\n",
    "for i_samp, sample in enumerate(annotations):\n",
    "    # if i_samp != 26:\n",
    "    #     continue\n",
    "    annot_claim = sample['annotated']\n",
    "    passage = sample['output']\n",
    "\n",
    "    # remove all <mark> </mark> tags and the content between the tags\n",
    "    unmarked_claim = remove_mark_tags(annot_claim)\n",
    "    \n",
    "    # preprocess claim - remove all non-error annotations\n",
    "    # print(annot_claim, '\\n')\n",
    "    clean_claim = remove_irrelevant_annotations(unmarked_claim, error_types)\n",
    "    \n",
    "    # perform sentence tokenization on clean claim\n",
    "    spacy_claim = spacy_nlp(clean_claim)\n",
    "    sentences = [sent.text for sent in spacy_claim.sents]\n",
    "\n",
    "    # cleanup edge cases\n",
    "    sentences = cleanup_lone_annot(sentences)\n",
    "    sentences = cleanup_leading_end_annot(sentences)\n",
    "    sentences = cleanup_dangling_start_annot(sentences)\n",
    "    \n",
    "    # iterate over sentences and extract errors\n",
    "    concat_passage = \"\"\n",
    "    for i_sent, sentence in enumerate(sentences):\n",
    "        # don't process sentences if 3 chars or less\n",
    "        if len(sentence) <= 3:\n",
    "            concat_passage += sentence\n",
    "            continue\n",
    "            \n",
    "        data_dict = {\"annotated_sentence\": sentence}\n",
    "        data_dict['annotated_passage'] = annot_claim\n",
    "        data_dict['id'] = id\n",
    "        data_dict['pid'] = i_samp\n",
    "        data_dict['sid'] = i_sent\n",
    "        # data_dict['passage'] = passage\n",
    "        # print(f\"sentence {i_sent}: {sentence}\\n\")\n",
    "        # process sentence\n",
    "        processed_sent, error_count_dict = process_annotations(sentence)\n",
    "        # if no errors and processed\n",
    "        # remove any remaining tags\n",
    "        processed_sent = re.sub(r'</?[^>]+>', '', processed_sent)\n",
    "        if len(concat_passage) == 0:\n",
    "            concat_passage += processed_sent\n",
    "        else:\n",
    "            concat_passage = concat_passage + \" \" + processed_sent\n",
    "        data_dict['sentence'] = processed_sent\n",
    "        data_dict['error_count_dict'] = error_count_dict\n",
    "        # print(f\"processed sentence: {i_sent}: {processed_sent}\\n\")\n",
    "        processed_sentences.append({\"input_info\":data_dict})\n",
    "        \n",
    "        if error_count_dict:\n",
    "            data_dict['has_error'] = True\n",
    "        else:\n",
    "            data_dict['has_error'] = False\n",
    "        id+=1\n",
    "        \n",
    "    concat_passages.append({'pid': i_samp, \"concat_passage\": concat_passage })\n",
    "    \n",
    "\n",
    "# now add concatenated passage to each sentence data dictionary\n",
    "for i, obj in enumerate(processed_sentences):\n",
    "    pid = obj['input_info']['pid']\n",
    "    concat_passage = concat_passages[pid][\"concat_passage\"]\n",
    "    obj['input_info']['concat_passage'] = concat_passage\n",
    "\n",
    "# Write to a .jsonl file\n",
    "with open('./data/rarr-input_fava_bs_all_rev2.jsonl', 'w') as f:\n",
    "    for entry in processed_sentences:\n",
    "        f.write(json.dumps(entry) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe204311-e036-4c70-a6c2-0a1ef8380d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_info'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_sentences[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81872a43-35ec-4a60-9841-d84361e0e174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['annotated_sentence', 'annotated_passage', 'id', 'pid', 'sid', 'sentence', 'error_count_dict', 'has_error', 'concat_passage'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_sentences[1000]['input_info'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f2ebf50-cd32-4340-a18e-8dd5fe849812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'What are the Golden Lion movies at the Venice Film Festival whose production countries are in Asia?',\n",
       " 'output': 'The Golden Lion is the highest prize awarded at the Venice Film Festival, which is one of the most prestigious film festivals in the world. The festival has a long history of showcasing and rewarding films from diverse countries and cultures. Here are some Golden Lion movies at the Venice Film Festival whose production countries are in Asia:1. \"The Battle of Algiers\" (1966) - Directed by Gillo Pontecorvo, this film is a powerful depiction of the Algerian struggle for independence from France. The film was produced in Algeria, a country in North Africa, but it has been recognized as a classic of world cinema and has had a significant impact on filmmakers across the globe.2. \"Theriority of the Family\" (1974) - Directed by Ousmane Sembene, this film is a powerful critique of the traditional family structure in Senegal and the impact of colonialism on African societies. It was the first film produced in Senegal and was a groundbreaking work in African cinema.3. \"The Tin Drum\" (1979) - Directed by Volker Schlondorff, this film is an adaptation of the novel of the same name by Günter Grass. It tells the story of a young boy growing up in Danzig during World War II and is a powerful exploration of the tumultuous history of Europe during that period.4. \"A City of Sadness\" (1989) - Directed by Hou Hsiao-hsien, this film is a powerful depiction of the tumultuous history of Taiwan during the 20th century. It tells the story of a family struggling to come to terms with their past and the political upheaval around them.5. \"The Puppetmaster\" (1993) - Directed by Hou Hsiao-hsien, this film is a biographical drama about the life of Li Tien-lu, a famous puppeteer from Taiwan. It explores the themes of identity, art, and the role of the artist in society.6. \"Still Life\" (2006) - Directed by Jia Zhangke, this film is a powerful depiction of the impact of the Three Gorges Dam project on the lives of ordinary people in China. It tells the story of a group of people who are forced to relocate and start a new life in a new town.7. \"The Wind Whispers to the Trees\" (2011) - Directed by HamidReza Ghotbi, this film is a poetic exploration of the relationship between nature and human beings. It tells the story of a young boy who goes on a journey to find his lost father and discovers a deeper connection with the natural world.These are just a few examples of the many Golden Lion movies at the Venice Film Festival whose production countries are in Asia. The festival has a long history of showcasing and rewarding films from diverse countries and cultures, and Asian films have played a significant role in that tradition.',\n",
       " 'annotated': 'The Golden Lion is the highest prize awarded at the Venice Film Festival. There have been several movies from Asian countries that have won the Golden Lion over the years. Here are some examples:1. Rashomon (Japan, 1951): Directed by Akira Kurosawa, this iconic Japanese film won the Golden Lion at the 1951 Venice Film Festival. It is known for its innovative storytelling and influential use of multiple perspectives.<contradictory><delete>2. The Battle of Algiers (Italy/Algeria, 1966): Directed by Gillo Pontecorvo, this film was a joint production between Italy and Algeria. It won the Golden Lion in 1966 and is a powerful portrayal of the Algerian struggle for independence.</delete></contradictory>3. Brokeback Mountain (USA<entity><delete>/Taiwan</delete></entity>, 2005): Directed by Ang Lee, this American romantic drama was also co-produced with Taiwan. It won the Golden Lion at the 2005 Venice Film Festival and received critical acclaim for its portrayal of a forbidden relationship between two cowboys.4. Lust, Caution (Taiwan/China/USA, 2007): Directed by Ang Lee, this film is a joint production between Taiwan, China, and the USA. It won the Golden Lion in 2007 and is a provocative espionage thriller set in World War II-era Shanghai.<contradictory><delete>5. Lebanon (Israel, 2009): Directed by Samuel Maoz, this Israeli war film won the Golden Lion at the 2009 Venice Film Festival.</delete></contradictory> It is a powerful depiction of the Israeli invasion of Lebanon in 1982.<contradictory><delete>These are just a few examples of Asian films that have won the Golden Lion at the Venice Film Festival over the years.</delete></contradictory>',\n",
       " 'subject': 'venice film festival',\n",
       " 'dataset': 'instruction-following',\n",
       " 'model': 'llama'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5d0358-2792-41d2-8b12-96b602224f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to a .jsonl file\n",
    "with open('./data/rarr-input_fava_bs_all_rev2.jsonl', 'w') as f:\n",
    "    for entry in processed_sentences:\n",
    "        f.write(json.dumps(entry) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c4518d6-5ee6-4d35-be3d-a7ae02f841f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"/Users/jjr/Documents/data/rarr-rep/fava/annotations.json\", 'r') as file:\n",
    "    annotations = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7da065ab-e2b6-4bfd-8ac4-ab54821a024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_mark = 0\n",
    "mark_ids = []\n",
    "for i,sample in enumerate(annotations):\n",
    "    annot_claim = sample['annotated']\n",
    "    if \"<mark>\" in annot_claim:\n",
    "        num_mark += 1\n",
    "        mark_ids.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7621558-e8b3-40cb-8e09-f83faa4cfa9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mark_ids[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d45aa65e-a5df-4e05-9102-14cbb75b91f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Long Long Way is a novel written by Sebastian Barry. It was first published in 2005 by <entity><delete>Faber and Faber</delete><mark>\\tviking press</mark></entity>, a <entity><delete>British</delete><mark>american</mark></entity> publishing company. The novel tells the story of Willie Dunne, an Irish soldier who has a strong sense of loyalty to his country, but also feels torn between his Irishness and his loyalty to the British army during World War I. The novel is set against the backdrop of the Easter Rising in Dublin in 1916, and the subsequent Irish War of Independence. It explores themes of nationalism, identity, family, and sacrifice. The novel was critically acclaimed and received several literary awards, <unverifiable><delete>including the 2005 Dublin Literary Award</delete></unverifiable> and was shortlisted for the Man Booker Prize in the same year.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations[3]['annotated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a496f6a2-e807-4ba2-8dfa-a72d0b1866fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2712574183.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[24], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    for sample in fava_all:\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "for sample in fava_all:\n",
    "    pas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "d5e4115f-b03a-4cd1-98c1-6f8afd2a0efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write to a .jsonl file\n",
    "# with open('./data/rarr-input_fava_bs.jsonl', 'w') as f:\n",
    "#     for entry in processed_sentences:\n",
    "#         f.write(json.dumps(entry) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd90e144-bf99-4f09-a53e-a79176cbf18a",
   "metadata": {},
   "source": [
    "# Analyze BS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcfc63e2-f742-4ff2-af7d-ace0c064491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fava_bs_all = []\n",
    "with open('./data/rarr-input_fava_bs_all_rev2.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        fava_bs_all.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebdecf6d-3a34-4f76-a09b-cad70da3c643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5150"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fava_bs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93caa331-750f-429f-8e47-b5def0d7ef1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotated_sentence': 'It was first published in 2005 by <entity>Faber and Faber</entity>, a <entity>British</entity> publishing company.',\n",
       " 'annotated_passage': 'A Long Long Way is a novel written by Sebastian Barry. It was first published in 2005 by <entity><delete>Faber and Faber</delete><mark>\\tviking press</mark></entity>, a <entity><delete>British</delete><mark>american</mark></entity> publishing company. The novel tells the story of Willie Dunne, an Irish soldier who has a strong sense of loyalty to his country, but also feels torn between his Irishness and his loyalty to the British army during World War I. The novel is set against the backdrop of the Easter Rising in Dublin in 1916, and the subsequent Irish War of Independence. It explores themes of nationalism, identity, family, and sacrifice. The novel was critically acclaimed and received several literary awards, <unverifiable><delete>including the 2005 Dublin Literary Award</delete></unverifiable> and was shortlisted for the Man Booker Prize in the same year.',\n",
       " 'id': 24,\n",
       " 'pid': 3,\n",
       " 'sid': 1,\n",
       " 'sentence': 'It was first published in 2005 by Faber and Faber, a British publishing company.',\n",
       " 'error_count_dict': {'entity': 2},\n",
       " 'has_error': True,\n",
       " 'concat_passage': 'A Long Long Way is a novel written by Sebastian Barry. It was first published in 2005 by Faber and Faber, a British publishing company. The novel tells the story of Willie Dunne, an Irish soldier who has a strong sense of loyalty to his country, but also feels torn between his Irishness and his loyalty to the British army during World War I. The novel is set against the backdrop of the Easter Rising in Dublin in 1916, and the subsequent Irish War of Independence. It explores themes of nationalism, identity, family, and sacrifice. The novel was critically acclaimed and received several literary awards, including the 2005 Dublin Literary Award and was shortlisted for the Man Booker Prize in the same year.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fava_bs_all[24]['input_info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "211fa2e2-fa79-49e9-ab39-ff1127f7d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the split sentences\n",
    "sentences_all = []\n",
    "\n",
    "for sample in fava_bs_all:\n",
    "    sentence = sample['input_info']['sentence']\n",
    "    annot_sentence = sample['input_info']['annotated_sentence']\n",
    "    sentences_all.append({'as':annot_sentence, 's':sentence})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a3a3682-3a28-4f87-b44b-97e2cefe695b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['annotated_sentence', 'annotated_passage', 'id', 'pid', 'sid', 'sentence', 'error_count_dict', 'has_error', 'concat_passage'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fava_bs_all[0]['input_info'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc836575-3ad6-4437-8aaf-4f5f3b14492a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5185"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32dc4b5a-72d5-4e00-abee-f4979f8cbf41",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'short_sents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mshort_sents\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'short_sents' is not defined"
     ]
    }
   ],
   "source": [
    "short_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ccafb971-ed51-4e8a-881d-cbe6c79c57b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the split sentences\n",
    "passages_all = [{}]\n",
    "\n",
    "for sample in fava_bs_all:\n",
    "    passage = sample['input_info']['concat_passage']\n",
    "    passages_all.append(passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "89e841d2-ca56-4657-806e-bb561b4410ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in passages_all:\n",
    "    if \"*\" in p:\n",
    "        print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8195190b-172f-4709-9432-1171c6ed0070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29a07022-6117-4b18-b041-5e7d1ebd319e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annot sent: 1634:\n",
      "sentence: 1634:\n",
      "annot sent: 1634:\n",
      "sentence: 1634:\n",
      "annot sent: <invented>\"The\n",
      "sentence: \"The\n",
      "annot sent: Sure!\n",
      "sentence: Sure!\n",
      "annot sent: 1634:\n",
      "sentence: 1634:\n",
      "annot sent: Sure!\n",
      "sentence: Sure!\n",
      "annot sent: 1634:\n",
      "sentence: 1634:\n",
      "annot sent: Flag:\n",
      "sentence: Flag:\n",
      "annot sent: <entity> *\n",
      "sentence:  *\n",
      "annot sent: Sure!\n",
      "sentence: Sure!\n",
      "annot sent: 2010.</entity>\n",
      "sentence: 2010.\n"
     ]
    }
   ],
   "source": [
    "short_sents = 0\n",
    "for sentence in sentences_all:\n",
    "    sent = sentence['s']\n",
    "    annot_sent = sentence['as']\n",
    "    if len(sent) <= 5:\n",
    "        short_sents += 1\n",
    "        print(f\"annot sent: {annot_sent}\")\n",
    "        print(f\"sentence: {sent}\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d898cce-59f2-4a4d-87af-a17e5db10913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['annotated_sentence', 'annotated_passage', 'id', 'cid', 'sid', 'passage', 'sentence', 'error_count_dict', 'has_error'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fava_bs_all[0]['input_info'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "d2737eda-843d-4709-9f7e-f961da666172",
   "metadata": {},
   "outputs": [],
   "source": [
    "fava_bs = []\n",
    "with open('./data/rarr-input_fava_bs.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        fava_bs.append(json.loads(line))\n",
    "\n",
    "# number of sentences\n",
    "num_sentences = len(fava_bs)\n",
    "error_count_total_dict = {key:0 for key in error_types}\n",
    "num_errors_total = 0\n",
    "for sample in fava_bs:\n",
    "    error_count_dict = sample['input_info']['error_count_dict']\n",
    "    num_errors = sum(error_count_dict.values())\n",
    "    num_errors_total+=num_errors\n",
    "    for error_type, count in error_count_dict.items():\n",
    "        error_count_total_dict[error_type] += count\n",
    "# number of sentences with > 1 error\n",
    "# number of errors by type\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "f6ea7f0d-fe72-40d1-9917-965666eccf4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contradictory': 159,\n",
       " 'relation': 24,\n",
       " 'entity': 460,\n",
       " 'invented': 98,\n",
       " 'unverifiable': 130,\n",
       " 'subjective': 125}"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_count_total_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "908c2ea2-01d6-47a7-b364-58e153e78d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "passage_error_count_total_dict = {'entity': 487,\n",
    "                                  'contradictory': 174,\n",
    "                                  'unverifiable': 151,\n",
    "                                  'invented': 150,\n",
    "                                  'subjective': 138,\n",
    "                                  'relation': 32}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "71894f0a-8586-46fb-b639-85a912d080e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences in FAVA BS: 5256\n",
      "number of errors in FAVA BS: 996\n",
      "number of errors in FAVA BP: 1132\n",
      "number of stranded errors in FAVA BS: 136\n",
      "errors by error type:\n",
      "\tcontradictory: 159\n",
      "\trelation: 24\n",
      "\tentity: 460\n",
      "\tinvented: 98\n",
      "\tunverifiable: 130\n",
      "\tsubjective: 125\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of sentences in FAVA BS: {num_sentences}\")\n",
    "print(f\"number of errors in FAVA BS: {num_errors_total}\")\n",
    "print(f\"number of errors in FAVA BP: {1132}\")\n",
    "print(f\"number of stranded errors in FAVA BS: {1132-num_errors_total}\")\n",
    "print(\"errors by error type:\")\n",
    "for error_type, count in error_count_total_dict.items():\n",
    "    print(f\"\\t{error_type}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892ffb76-a486-49e5-8860-02000619b901",
   "metadata": {},
   "source": [
    "# BS Stratified Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "75297f4e-dc55-4423-acfb-40878b033c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert to dataframe\n",
    "# import pandas as pd\n",
    "\n",
    "# # Define error mapping\n",
    "# error_mapping = {\n",
    "#     'contradictory': 'e1',\n",
    "#     'relation': 'e2',\n",
    "#     'entity': 'e3',\n",
    "#     'invented': 'e4',\n",
    "#     'unverifiable': 'e5',\n",
    "#     'subjective': 'e6'\n",
    "# }\n",
    "\n",
    "# # Create a list to hold the DataFrame rows\n",
    "# rows = []\n",
    "\n",
    "# # Loop through each item in the dataset\n",
    "# for item in processed_sentences:\n",
    "#     row_info = item['input_info']\n",
    "#     row = {\n",
    "#         'id': row_info['id'],\n",
    "#         'e1': row_info['error_count_dict'].get('contradictory', 0),\n",
    "#         'e2': row_info['error_count_dict'].get('relation', 0),\n",
    "#         'e3': row_info['error_count_dict'].get('entity', 0),\n",
    "#         'e4': row_info['error_count_dict'].get('invented', 0),\n",
    "#         'e5': row_info['error_count_dict'].get('unverifiable', 0),\n",
    "#         'e6': row_info['error_count_dict'].get('subjective', 0),\n",
    "#     }\n",
    "#     rows.append(row)\n",
    "\n",
    "# # Create DataFrame\n",
    "# df = pd.DataFrame(rows, columns=['id', 'e1', 'e2', 'e3', 'e4', 'e5', 'e6'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "9fad2acd-e006-431e-ad42-c7f68ae46657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e2 method 1: 0.0046\n",
      "e2 method 2: 0.0045662100456621 \n",
      "\n",
      "e4 method 1: 0.0164\n",
      "e4 method 2: 0.016362252663622526 \n",
      "\n",
      "e6 method 1: 0.0221\n",
      "e6 method 2: 0.02207001522070015 \n",
      "\n",
      "e5 method 1: 0.0219\n",
      "e5 method 2: 0.021879756468797563 \n",
      "\n",
      "e1 method 1: 0.0263\n",
      "e1 method 2: 0.026255707762557076 \n",
      "\n",
      "e3 method 1: 0.0712\n",
      "e3 method 2: 0.07115677321156773 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # convert to binary\n",
    "# df_binary = df.copy()\n",
    "# df_binary.loc[:, 'e1':'e6'] = (df_binary.loc[:, 'e1':'e6'] >= 1).astype(int)\n",
    "\n",
    "# for col in ['e2', 'e4', 'e6', 'e5', 'e1', 'e3']:\n",
    "#     print(col, 'method 1:', round(df_binary[col].mean(), 4))\n",
    "#     print(col, 'method 2:', df_binary[df_binary[col] == 1][col].sum()/len(df_binary), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "1dcdb63b-3659-4bf5-9bd6-54be8741e6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# # Desired sample size\n",
    "# sample_size = 460\n",
    "\n",
    "# # Initialize DataFrame to store the sampled indices\n",
    "# sampled_indices = pd.Index([])\n",
    "\n",
    "# sample_fraction = sample_size / len(df_binary)\n",
    "\n",
    "# # Stratified sampling for each error type\n",
    "# for error in ['e2', 'e4', 'e6', 'e5', 'e1', 'e3']:\n",
    "#     splitter = StratifiedShuffleSplit(n_splits=1, test_size=sample_fraction, random_state=42)\n",
    "#     for _, test_index in splitter.split(df_binary, df_binary[error]):\n",
    "#         sampled_indices = sampled_indices.union(test_index)\n",
    "\n",
    "# # Deduplicate indices in case some rows are selected multiple times\n",
    "# sampled_indices = sampled_indices.unique()\n",
    "\n",
    "# # If we have more samples than needed, randomly drop the excess\n",
    "# if len(sampled_indices) > sample_size:\n",
    "#     sampled_indices = np.random.choice(sampled_indices, size=sample_size, replace=False)\n",
    "\n",
    "# # Get the stratified sample DataFrame\n",
    "# stratified_sample = df_binary.loc[sampled_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "11dce8f0-9adf-41cc-961e-df48575da0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e2 method 1: 0.0022\n",
      "e4 method 1: 0.0217\n",
      "e6 method 1: 0.0196\n",
      "e5 method 1: 0.0196\n",
      "e1 method 1: 0.0261\n",
      "e3 method 1: 0.063\n"
     ]
    }
   ],
   "source": [
    "# for col in ['e1', 'e2', 'e3', 'e4', 'e5', 'e6']:\n",
    "#     print(col, 'method 1:', round(stratified_sample[col].mean(), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "3ecdc6af-9789-4c0a-a2dd-3a504bb6c5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified_sample_ids = stratified_sample['id'].tolist()\n",
    "# stratified_sample_ids = sorted(stratified_sample_ids)\n",
    "# processed_sentences_sample = [processed_sentences[i] for i in stratified_sample_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "4cd487a8-aa92-4e0a-8b39-bae4979e22de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write to a .jsonl file\n",
    "# with open('./data/rarr-input_fava_bs.jsonl', 'w') as f:\n",
    "#     for entry in processed_sentences_sample:\n",
    "#         f.write(json.dumps(entry) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "0384cc85-3c15-44c4-81a5-28168f3a7dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write to a .jsonl file\n",
    "# with open('./data/rarr-input_fava_bs_all.jsonl', 'w') as f:\n",
    "#     for entry in processed_sentences:\n",
    "#         f.write(json.dumps(entry) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "2bcf4956-b182-4ed0-8100-cb3df31acc16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5256"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(processed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "4badc110-3b72-488c-81c6-7faeb89d2694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_sentences_errors = [sample for sample in processed_sentences if sample['input_info']['has_error']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "f9f10213-17f9-470c-b1a6-7c7e383c870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write to a .jsonl file\n",
    "# with open('./data/rarr-input_fava_bs_all_errors.jsonl', 'w') as f:\n",
    "#     for entry in processed_sentences_errors:\n",
    "#         f.write(json.dumps(entry) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "f7fa7577-0f01-4ec9-9ab0-c49dc99eec8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>e1</th>\n",
       "      <th>e2</th>\n",
       "      <th>e3</th>\n",
       "      <th>e4</th>\n",
       "      <th>e5</th>\n",
       "      <th>e6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5176</th>\n",
       "      <td>5176</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5178</th>\n",
       "      <td>5178</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5179</th>\n",
       "      <td>5179</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5180</th>\n",
       "      <td>5180</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5241</th>\n",
       "      <td>5241</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>786 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  e1  e2  e3  e4  e5  e6\n",
       "6        6   0   0   0   0   0   1\n",
       "10      10   0   0   0   0   1   0\n",
       "11      11   0   1   0   0   0   0\n",
       "24      24   0   0   1   0   0   0\n",
       "28      28   0   0   0   0   1   0\n",
       "...    ...  ..  ..  ..  ..  ..  ..\n",
       "5176  5176   0   0   1   0   0   0\n",
       "5178  5178   0   0   1   0   0   0\n",
       "5179  5179   0   0   1   0   0   0\n",
       "5180  5180   0   0   1   0   0   0\n",
       "5241  5241   0   0   1   0   0   0\n",
       "\n",
       "[786 rows x 7 columns]"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # processed_sentences -> errors only\n",
    "# mask = (df_binary.e1 == 1) | (df_binary.e2 == 1) | (df_binary.e3 == 1) | (df_binary.e4 == 1) | (df_binary.e5 == 1) | (df_binary.e6 == 1)\n",
    "# df_binary[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3a4585-0267-44bf-ac1b-09547a30c070",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## -> helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e95ed76-2ef9-4c54-82ad-cf3b90fe5309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def display_rarr_output(idx_start: int, \n",
    "                        idx_end:int,\n",
    "                       annotations: List[Dict[str,any]],\n",
    "                       output: List[Dict[str,any]],\n",
    "                        display_filter: List[int]=None,\n",
    "                       show_revision_steps: bool = True):\n",
    "    for idx in range(idx_start, idx_end, 1):\n",
    "    # for idx in range(0,224):\n",
    "        \n",
    "        if display_filter and idx not in display_filter:\n",
    "            continue\n",
    "        print(f\"ID:{idx}\\n\")\n",
    "        show_revision_steps = True\n",
    "        \n",
    "        annotated_gold = annotations[idx][\"annotated\"]\n",
    "        \n",
    "        # annotated_rarr = output_8b[idx][\"result\"][\"annotations\"]\n",
    "        \n",
    "        claim = output[idx][\"result\"][\"claim\"]\n",
    "        print(f\"--------------CLAIM--------------\\n{claim}\\n\")\n",
    "        \n",
    "        try:\n",
    "            questions = output[idx][\"result\"][\"agreement_gates\"]\n",
    "            if questions is None: questions = []\n",
    "        except KeyError:\n",
    "            questions = []\n",
    "        print(f\"--------------QUESTIONS--------------\\nNo. of questions generated: {len(questions)}\\n\")\n",
    "        \n",
    "        try:\n",
    "            if show_revision_steps:\n",
    "                print(\"--------------REVISIONS--------------\")\n",
    "                agreement_gates = output[idx][\"result\"][\"agreement_gates\"]\n",
    "                no_revisions = True\n",
    "                for agr,rev in zip(agreement_gates, output[idx][\"result\"][\"revisions\"]):\n",
    "                    gate_status = agr[\"is_open\"]\n",
    "                    if gate_status:\n",
    "                        no_revisions = False\n",
    "                        reason = agr[\"reason\"]\n",
    "                        print(f\"-->Edit Gate Open?: {gate_status}\\n-->reason: {reason}\\n\")\n",
    "                        print(rev['revised_claim'], \"\\n\")\n",
    "                if no_revisions:\n",
    "                    print(\"No Revisions\")\n",
    "        except (TypeError, KeyError) as e:\n",
    "            print(\"\\n\", \"Malfunction - no questions\", \"\\n\")\n",
    "        \n",
    "        revised_claim = output[idx][\"result\"][\"revised_claim\"]\n",
    "        print(f\"--------------REVISED CLAIM--------------:\\n{revised_claim}\\n\")\n",
    "        \n",
    "        # print(\"--------------ERROR ANNOTATIONS--------------\")\n",
    "        # print(f\"gold:{results[idx]['gold']}\\n\")\n",
    "        # print(f\"rarr:{results[idx]['rarr']}\\n\")\n",
    "        # print(f\"****** gold annotated ******:\\n{annotated_gold}\\n\\n****** rarr annotated ******:\\n{annotated_rarr}\\n\")\n",
    "        print(\"********************************************************************************************************\")\n",
    "        print(\"********************************************************************************************************\")\n",
    "        print(\"********************************************************************************************************\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rarr-rep",
   "language": "python",
   "name": "rarr-rep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
